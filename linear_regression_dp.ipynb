{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data  # TODO\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 读取数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据形状：\n",
    "$y \\in \\mathbb{R}^1, x \\in \\mathbb{R}^2$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sample_num = 1000\n",
    "lr = 0.01\n",
    "num_epochs = 20\n",
    "epsilon=10\n",
    "delta=0.01\n",
    "max_grad_norm=5.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, sample_num)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# 生成训练数据序列\n",
    "def load_data(data_array, batch_size):\n",
    "    # dataset = data.TensorDataset(data_array) # TODO\n",
    "    dataset = data.TensorDataset(*data_array)  # TODO when use *?\n",
    "    return data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "data_iter = load_data((features, labels), batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "from torch import nn\n",
    "\n",
    "# 构建网络\n",
    "net = nn.Sequential(nn.Linear(2, 1)) # Sequential用于堆砌不同的层——这里只有一个线性层，输入是2维，输出是1维\n",
    "\n",
    "# 参数初始化\n",
    "net[0].weight.data.normal_(0, 0.01)  # weights初始化为标准正态分布；这里是直接replace，而不是新生成的内存空间\n",
    "net[0].bias.data.fill_(0) # bias初始化为0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**OPACUS**\n",
    "调用opacus的ModuleValidator对象来判断搭建的模型是否可以DP化（BN这种层是无法DP化的，因为这里假设多个sample独立加噪音，BN会导致单个数据的改变影响到多个样本梯度，而不是一个）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opacus.validators import ModuleValidator\n",
    "\n",
    "errors = ModuleValidator.validate(net, strict=False)\n",
    "print(errors[-5:])\n",
    "net = ModuleValidator.fix(net)\n",
    "ModuleValidator.validate(net, strict=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# 启动optimizer\n",
    "optimizer = torch.optim.SGD(params=net.parameters(), lr=lr) # 第一个参数是网络中待优化的参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**OPACUS**\n",
    "调用opacus的PrivacyEngine对象，将net、optimizer、data_iter这几个实例重新wrap一下"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\torch_gpu\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from opacus import PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(accountant=\"rdp\") # 选择隐私统计技术为RDP\n",
    "# OPTION1：给定epsilon的情况，设定net, optimizer, data_iter；除了这三个东西，还需要传入的参数：\n",
    "# 1. epsilon、delta以及C（clip的参数）\n",
    "# 2. epoch（用于指导epsilon的compose）\n",
    "net, optimizer, data_iter = privacy_engine.make_private_with_epsilon(\n",
    "    module=net,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=data_iter,\n",
    "    epochs=num_epochs,\n",
    "    target_epsilon=epsilon,\n",
    "    target_delta=delta,\n",
    "    max_grad_norm=max_grad_norm,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# 构建loss\n",
    "loss=nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:9.21086311340332\n",
      "epsilon:2.795883159554287, delta:0.01\n",
      "epoch:1, loss:1.6003155708312988\n",
      "epsilon:3.5034487725000676, delta:0.01\n",
      "epoch:2, loss:0.07424916326999664\n",
      "epsilon:4.071951297116898, delta:0.01\n",
      "epoch:3, loss:0.004222839139401913\n",
      "epsilon:4.571115656894907, delta:0.01\n",
      "epoch:4, loss:0.00020390874124132097\n",
      "epsilon:5.0164283670466565, delta:0.01\n",
      "epoch:5, loss:0.0002676925214473158\n",
      "epsilon:5.446633502303051, delta:0.01\n",
      "epoch:6, loss:0.0010992612224072218\n",
      "epsilon:5.838976219786288, delta:0.01\n",
      "epoch:7, loss:0.0002140703290933743\n",
      "epsilon:6.213276276203157, delta:0.01\n",
      "epoch:8, loss:0.0006299919332377613\n",
      "epsilon:6.587576332620027, delta:0.01\n",
      "epoch:9, loss:0.00040755074587650597\n",
      "epsilon:6.937089880916106, delta:0.01\n",
      "epoch:10, loss:0.0006456022965721786\n",
      "epsilon:7.265151831433438, delta:0.01\n",
      "epoch:11, loss:0.0007640901603735983\n",
      "epsilon:7.593213781950772, delta:0.01\n",
      "epoch:12, loss:0.000576560792978853\n",
      "epsilon:7.9212757324681045, delta:0.01\n",
      "epoch:13, loss:0.0013757436536252499\n",
      "epsilon:8.249337682985438, delta:0.01\n",
      "epoch:14, loss:0.00035631953505799174\n",
      "epsilon:8.547694020645727, delta:0.01\n",
      "epoch:15, loss:0.0011936636874452233\n",
      "epsilon:8.8368203430127, delta:0.01\n",
      "epoch:16, loss:0.0004922644584439695\n",
      "epsilon:9.125946665379674, delta:0.01\n",
      "epoch:17, loss:0.0004049184499308467\n",
      "epsilon:9.415072987746646, delta:0.01\n",
      "epoch:18, loss:0.0009433391387574375\n",
      "epsilon:9.70419931011362, delta:0.01\n",
      "epoch:19, loss:0.00042169480002485216\n",
      "epsilon:9.993325632480593, delta:0.01\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        # loss = net(X) # TODO\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(net(X), y)\n",
    "        # optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    # l = loss(net(features), labels)  # TODO all data\n",
    "    print(f\"epoch:{epoch}, loss:{l}\")\n",
    "    epsilon = privacy_engine.get_epsilon(delta)\n",
    "    print(f\"epsilon:{epsilon}, delta:{delta}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "检查训练得到的模型参数和真实值的差距"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9953, -3.4089]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([4.2111], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Get parameters of the net\n",
    "# w = net[0].weight.data # The wrapped net does not allow subscript\n",
    "# b = net[0].bias.data\n",
    "for param in net.parameters(): # parameters() return a generator\n",
    "    print(param)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
